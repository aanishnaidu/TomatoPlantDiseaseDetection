{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import keras\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from os import listdir\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.layers.core import Activation, Flatten, Dropout, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, svm, metrics\n",
    "from keras import backend as K\n",
    "import pickle\n",
    "import joblib\n",
    "from keras.models import model_from_json\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from keras.applications import ResNet50\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = \"true\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ph1=r\"C:\\Users\\aanis\\Desktop\\plantvillage1\\aa\\segmented\\tomato\"\n",
    "li=0\n",
    "x=[]\n",
    "y=[]\n",
    "for i in os.listdir(ph1):\n",
    "    #lab=[0,0,0,0,0,0,0,0,0,0]\n",
    "    #lab[li]=1\n",
    "    ph2=ph1+f'\\{i}'\n",
    "    #print(ph2)\n",
    "    \n",
    "    for j in os.listdir(ph2):\n",
    "        #print(j)\n",
    "        ph3=ph2+f'\\{j}'\n",
    "        #print(x,np.array(cv2.imread(ph3)).reshape(256*256*3))\n",
    "        #print(lab)\n",
    "        x.append(np.array(cv2.imread(ph3)))\n",
    "        y.append(li)\n",
    "    li+=1    \n",
    "#print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aug = ImageDataGenerator(rotation_range=25, width_shift_range=0.1,height_shift_range=0.1, shear_range=0.2, zoom_range=0.2,horizontal_flip=True, fill_mode=\"nearest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(np.array(y).shape)\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.array(x), np.array(y), test_size=0.2, random_state = 42) \n",
    "del x\n",
    "del y\n",
    "print(x_train[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wpath=r\"C:\\Users\\aanis\\Downloads\\resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "\n",
    "xtrain=[]\n",
    "xtest=[]\n",
    "\n",
    "\n",
    "resnet = ResNet50(weights=wpath, include_top=False, input_shape=(256, 256, 3))\n",
    "#print(x_train.shape[0]/48)\n",
    "\n",
    "for i in range(x_train.shape[0]):\n",
    "    xtrain.append(resnet.predict(x_train[i:i+1]).reshape(8*8*2048))\n",
    "del x_train\n",
    "xtrain=np.array(xtrain)\n",
    "\n",
    "for i in range(x_test.shape[0]):\n",
    "    xtest.append(resnet.predict(x_test[i:i+1]).reshape(8*8*2048))\n",
    "del x_test\n",
    "xtest=np.array(xtest)\n",
    "\n",
    "\n",
    "print(xtest.shape)\n",
    "print(xtrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.save(\"xtrain_rsnet\",xtrain)\n",
    "np.save(\"ytrain_rsnet\",y_train)\n",
    "np.save(\"xtest_rsnet\",xtest)\n",
    "np.save(\"ytest_rsnet\",y_test)\n",
    "del xtrain\n",
    "del y_train\n",
    "del xtest\n",
    "del y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "(14527,)\n",
      "(3632,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm, datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as geek \n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "print(\"0\")\n",
    "# the array is loaded intob \n",
    "xtrain = geek.load('xtrain_rsnet.npy')\n",
    "ytrain = geek.load('ytrain_rsnet.npy')\n",
    "xtest = geek.load('xtest_rsnet.npy')\n",
    "ytest = geek.load('ytest_rsnet.npy')\n",
    "print(\"1\")\n",
    "xtrain=xtrain[:10000]\n",
    "ytrain=ytrain[:10000]\n",
    "ytest=ytest[:2200]\n",
    "xtest=xtest[:2200]\n",
    "\n",
    "print(ytrain.shape)\n",
    "print(ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling = MinMaxScaler(feature_range=(0,1)).fit(xtrain)\n",
    "xtrain = scaling.transform(xtrain)\n",
    "xtest = scaling.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"2\")\n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(xtrain, ytrain)\n",
    "print(\"ok\")\n",
    "acc = clf.score(xtest, ytest)\n",
    "print(\"Accuracy:\", acc)\n",
    "\n",
    "\n",
    "\n",
    "filename = 'qqqfinal.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,29):\n",
    "    print(str(i)+\"iter\")\n",
    "    clf.partial_fit(xtrain[500*i:((500*i)+500)], ytrain[500*i:((500*i)+500)])\n",
    "    acc = clf.score(xtest, ytest)\n",
    "    print(\"Accuracy:\", acc)\n",
    "    #filename = \"qqq__\"+str(i)+\".sav\"\n",
    "    #pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "scaling = MinMaxScaler(feature_range=(0,1)).fit(xtrain)\n",
    "xtrain = scaling.transform(xtrain)\n",
    "xtest = scaling.transform(xtest)\n",
    "print(\"2\")\n",
    "#clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "clf = SGDClassifier()\n",
    "print(\"ok\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbf_feature = RBFSampler(gamma='auto')\n",
    "x= rbf_feature.fit_transform(xtrain[:500])\n",
    "\n",
    "clf.fit(x, ytrain[:500])\n",
    "\n",
    "acc = clf.score(xtest, ytest)\n",
    "print(\"Accuracy:\", acc)\n",
    "\n",
    "\n",
    "\n",
    "filename = 'qqqfinal.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,29):\n",
    "    print(str(i)+\"iter\")\n",
    "    \n",
    "    rbf_feature = RBFSampler(gamma='auto')\n",
    "    x= rbf_feature.fit_transform(xtrain[500*i:(i+1)*500])\n",
    "    \n",
    "    clf.partial_fit(x, ytrain[500*i:((500*i)+500)])\n",
    "    acc = clf.score(xtest, ytest)\n",
    "    print(\"Accuracy:\", acc)\n",
    "    filename = \"qqq__\"+str(i)+\".sav\"\n",
    "    pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(xtrain, ytrain)\n",
    "\n",
    "acc = clf.score(xtest, ytest)\n",
    "print(\"Accuracy:\", acc)\n",
    "\n",
    "import pickle\n",
    "\n",
    "filename = 'finalresnetsvm.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "acc = clf.score(xtest, ytest)\n",
    "print(\"Accuracy:\", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = 'resnetsvm.sav'\n",
    "pickle.dump(clf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'resnetsvm.sav'\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(xtest[:1], ytest[:1])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
